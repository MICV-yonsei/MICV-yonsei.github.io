<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MICV</title>
    <link>http://localhost:1313/</link>
      <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <description>MICV</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_huc20aa2f6f5c0a6f78f1951b0621355e5_26767_512x512_fill_lanczos_center_3.png</url>
      <title>MICV</title>
      <link>http://localhost:1313/</link>
    </image>
    
    <item>
      <title>Example Event</title>
      <link>http://localhost:1313/event/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>http://localhost:1313/event/example/</guid>
      <description>&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://docs.hugoblox.com/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://docs.hugoblox.com/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including page elements such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two papers early-accepted for MICCAI 2024</title>
      <link>http://localhost:1313/post/2024-05-01-two-papers-early-accepted-for-miccai-2024/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-05-01-two-papers-early-accepted-for-miccai-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Funding : 우수신진연구</title>
      <link>http://localhost:1313/post/2024-04-03-funding-%EC%9A%B0%EC%88%98%EC%8B%A0%EC%A7%84%EC%97%B0%EA%B5%AC/</link>
      <pubDate>Wed, 03 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-04-03-funding-%EC%9A%B0%EC%88%98%EC%8B%A0%EC%A7%84%EC%97%B0%EA%B5%AC/</guid>
      <description>&lt;p&gt;NeuroGPT: 치매 진단보조를 위한 뇌영상 및 전자의무기록 중심 멀티모달 대화형 생성모델 개발&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Funding - LG전자 &amp; LG Display</title>
      <link>http://localhost:1313/post/2024-04-02-funding-lg%EC%A0%84%EC%9E%90--lg-display/</link>
      <pubDate>Tue, 02 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-04-02-funding-lg%EC%A0%84%EC%9E%90--lg-display/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Received 2023 Distinguished Faculty Award - Teaching</title>
      <link>http://localhost:1313/post/2024-04-01-received-2023-distinguished-faculty-award-teaching/</link>
      <pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-04-01-received-2023-distinguished-faculty-award-teaching/</guid>
      <description>&lt;p&gt;2023 우수업적교수상: 교육부문&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Funding - 서울혁신챌린지(본선)</title>
      <link>http://localhost:1313/post/2024-03-01-funding-%EC%84%9C%EC%9A%B8%ED%98%81%EC%8B%A0%EC%B1%8C%EB%A6%B0%EC%A7%80%EB%B3%B8%EC%84%A0/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-03-01-funding-%EC%84%9C%EC%9A%B8%ED%98%81%EC%8B%A0%EC%B1%8C%EB%A6%B0%EC%A7%80%EB%B3%B8%EC%84%A0/</guid>
      <description>&lt;p&gt;인공지능 기반의 안드로겐 탈모 진단 시스템 연구개발&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>EAGLE accepted to CVPR&#39;24 as a Highlight paper Congrats to co-authors Chanyoung Kim, Woojung Han, and Dayun Ju!</title>
      <link>http://localhost:1313/post/2024-02-01-cvpr24-our-work-eagle/</link>
      <pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-02-01-cvpr24-our-work-eagle/</guid>
      <description>&lt;p&gt;Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation accepted to CVPR&#39;24 as a Highlight paper. Congrats to co-authors Chanyoung Kim, Woojung Han, and Dayun Ju!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Yeongjun Jun, Sujung Hong, Junhyeok Kim, Jiwoo Park, and Suhyun Kim join our lab</title>
      <link>http://localhost:1313/post/2024-01-01-yeongjun-jun-sujung-hong-junhyeok-kim-jiwoo-park-and-suhyun-kim-join-our-lab/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-01-01-yeongjun-jun-sujung-hong-junhyeok-kim-jiwoo-park-and-suhyun-kim-join-our-lab/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Funding - 서울혁신챌린지(예선)</title>
      <link>http://localhost:1313/post/2023-08-02-funding-%EC%84%9C%EC%9A%B8%ED%98%81%EC%8B%A0%EC%B1%8C%EB%A6%B0%EC%A7%80%EC%98%88%EC%84%A0/</link>
      <pubDate>Wed, 02 Aug 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2023-08-02-funding-%EC%84%9C%EC%9A%B8%ED%98%81%EC%8B%A0%EC%B1%8C%EB%A6%B0%EC%A7%80%EC%98%88%EC%84%A0/</guid>
      <description>&lt;p&gt;인공지능 기반의 안드로겐 탈모 진단 시스템 연구개발&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accepted to Medical Image Analysis</title>
      <link>http://localhost:1313/post/2023-08-01-accepted-to-medical-image-analysis/</link>
      <pubDate>Tue, 01 Aug 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2023-08-01-accepted-to-medical-image-analysis/</guid>
      <description>&lt;p&gt;Congrats to Mahbaneh!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Funding - 한국연구재단 / 기초연구실</title>
      <link>http://localhost:1313/post/2023-07-02-funding-%ED%95%9C%EA%B5%AD%EC%97%B0%EA%B5%AC%EC%9E%AC%EB%8B%A8---%EA%B8%B0%EC%B4%88%EC%97%B0%EA%B5%AC%EC%8B%A4/</link>
      <pubDate>Sun, 02 Jul 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2023-07-02-funding-%ED%95%9C%EA%B5%AD%EC%97%B0%EA%B5%AC%EC%9E%AC%EB%8B%A8---%EA%B8%B0%EC%B4%88%EC%97%B0%EA%B5%AC%EC%8B%A4/</guid>
      <description>&lt;p&gt;CT 영상 화질개선을 위한 인공지능 연구실&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Funding - 한국연구재단/데이터 기반 디지털 바이오 선도사업</title>
      <link>http://localhost:1313/post/2023-07-01-funding-%ED%95%9C%EA%B5%AD%EC%97%B0%EA%B5%AC%EC%9E%AC%EB%8B%A8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B8%B0%EB%B0%98-%EB%94%94%EC%A7%80%ED%84%B8-%EB%B0%94%EC%9D%B4%EC%98%A4-%EC%84%A0%EB%8F%84%EC%82%AC%EC%97%85/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2023-07-01-funding-%ED%95%9C%EA%B5%AD%EC%97%B0%EA%B5%AC%EC%9E%AC%EB%8B%A8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B8%B0%EB%B0%98-%EB%94%94%EC%A7%80%ED%84%B8-%EB%B0%94%EC%9D%B4%EC%98%A4-%EC%84%A0%EB%8F%84%EC%82%AC%EC%97%85/</guid>
      <description>&lt;p&gt;바이오 빅데이터 기반 당뇨병 및 합병증 정밀 의료를 위한 AI 플랫폼 및 모델 개발&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Advancing Text-Driven Chest X-Ray Generation with Policy-Based Reinforcement Learning</title>
      <link>http://localhost:1313/publication/2024-miccai-cxrl/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/2024-miccai-cxrl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slice-Consistent 3D Volumetric Brain CT-to-MRI Translation with 2D Brownian Bridge Diffusion Model</title>
      <link>http://localhost:1313/ct2mri2024/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/ct2mri2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>WoLF: Large Language Model Framework for CXR Understanding</title>
      <link>http://localhost:1313/publication/2024-arxiv-wolf/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/2024-arxiv-wolf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dayun Ju and Chanyoung Kim join our lab</title>
      <link>http://localhost:1313/post/2023-03-01-dayun-ju-and-chanyoung-kim-join-our-lab/</link>
      <pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2023-03-01-dayun-ju-and-chanyoung-kim-join-our-lab/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accepted for Machine Learning on theoretical domain generalization</title>
      <link>http://localhost:1313/post/2023-02-01-accepted-for-machine-learning-on-theoretical-domain-generalization/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2023-02-01-accepted-for-machine-learning-on-theoretical-domain-generalization/</guid>
      <description>&lt;p&gt;Congrats to Anthony!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation</title>
      <link>http://localhost:1313/publication/2024-cvpr-eagle/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/2024-cvpr-eagle/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gayoon Choi, Taejin Jeong, and Jeahoon Joo join our lab</title>
      <link>http://localhost:1313/post/2023-01-02-gayoon-choi-taejin-jeong-and-jeahoon-joo-join-our-lab/</link>
      <pubDate>Mon, 02 Jan 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2023-01-02-gayoon-choi-taejin-jeong-and-jeahoon-joo-join-our-lab/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accepted for ISBI 2023</title>
      <link>http://localhost:1313/post/2023-01-01-accepted-for-isbi-2023/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2023-01-01-accepted-for-isbi-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Member</title>
      <link>http://localhost:1313/people/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper won the Best Paper Award at UAI 2022</title>
      <link>http://localhost:1313/post/2022-08-01-our-paper-won-the-best-paper-award-at-uai-2022/</link>
      <pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2022-08-01-our-paper-won-the-best-paper-award-at-uai-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Yumin Kim, Seil Kang, Kyobin Choo, Hyunjin Kim, and Donghyun Kim join our lab</title>
      <link>http://localhost:1313/post/2022-07-01-yumin-kim-seil-kang-kyobin-choo-hyunjin-kim-and-donghyun-kim-join-our-lab/</link>
      <pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2022-07-01-yumin-kim-seil-kang-kyobin-choo-hyunjin-kim-and-donghyun-kim-join-our-lab/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accepted for UAI 2022, Eindhoven, the Netherlands for an Oral Presentation</title>
      <link>http://localhost:1313/post/2022-05-01-accepted-for-uai-2022-eindhoven-the-netherlands-for-an-oral-presentation/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2022-05-01-accepted-for-uai-2022-eindhoven-the-netherlands-for-an-oral-presentation/</guid>
      <description>&lt;p&gt;Congrats to Anthony!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accepted for IJCAI 2022, Vienna</title>
      <link>http://localhost:1313/post/2022-04-01-accepted-for-ijcai-2022-vienna/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2022-04-01-accepted-for-ijcai-2022-vienna/</guid>
      <description>&lt;p&gt;Congrats to Xingchen and Anthony!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Joining as an Assistant Professor in the Department of Artificial Intelligence at Yonsei University</title>
      <link>http://localhost:1313/post/2022-03-01-joining-as-an-assistant-professor-in-the-department-of-artificial-intelligence-at-yonsei-university/</link>
      <pubDate>Sun, 13 Mar 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2022-03-01-joining-as-an-assistant-professor-in-the-department-of-artificial-intelligence-at-yonsei-university/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Yujin Yang and Woojung Han join our lab</title>
      <link>http://localhost:1313/post/2022-03-03-yujin-yang-and-woojung-han-join-our-lab/</link>
      <pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2022-03-03-yujin-yang-and-woojung-han-join-our-lab/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accepted for Findings of ACL 2022</title>
      <link>http://localhost:1313/post/2022-03-02-accepted-for-findings-of-acl-2022/</link>
      <pubDate>Wed, 02 Mar 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2022-03-02-accepted-for-findings-of-acl-2022/</guid>
      <description>&lt;p&gt;Congrats to Anthony!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accepted for NeuroImage</title>
      <link>http://localhost:1313/post/2021-10-08-accepted-for-neuroimage/</link>
      <pubDate>Fri, 08 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2021-10-08-accepted-for-neuroimage/</guid>
      <description>&lt;p&gt;Congrats to Mahbaneh!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accepted for The First Workshop on Computer Vision for Automated Medical Diagnosis @ ICCV 2021</title>
      <link>http://localhost:1313/post/2021-08-14-accepted-for-the-first-workshop-on-computer-vision-for-automated-medical-diagnosis-@-iccv-2021/</link>
      <pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2021-08-14-accepted-for-the-first-workshop-on-computer-vision-for-automated-medical-diagnosis-@-iccv-2021/</guid>
      <description>&lt;p&gt;Congrats to Mahbaneh!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accepted for MICCAI 2021, Virtual</title>
      <link>http://localhost:1313/post/2021-06-07-accepted-for-miccai-2021-virtual/</link>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2021-06-07-accepted-for-miccai-2021-virtual/</guid>
      <description>&lt;p&gt;Congrats to Anthony!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accepted for MIDL 2021, Virtual</title>
      <link>http://localhost:1313/post/2021-05-16-accepted-for-midl-2021-virtual/</link>
      <pubDate>Sun, 16 May 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2021-05-16-accepted-for-midl-2021-virtual/</guid>
      <description>&lt;p&gt;Congrats to Shibo!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accepted for AAIC 2021, Denver, USA</title>
      <link>http://localhost:1313/post/2021-03-11-accepted-for-aaic-2021-denver-usa/</link>
      <pubDate>Thu, 11 Mar 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2021-03-11-accepted-for-aaic-2021-denver-usa/</guid>
      <description>&lt;p&gt;Congrats to Mahbaneh!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Two Full Papers Accepted for ISBI 2021, Virtual</title>
      <link>http://localhost:1313/post/2021-02-27-two-full-papers-accepted-for-isbi-2021-virtual/</link>
      <pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2021-02-27-two-full-papers-accepted-for-isbi-2021-virtual/</guid>
      <description>&lt;p&gt;Congrats to Anthony and Xingchen!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accepted for IPMI 2021, Virtual</title>
      <link>http://localhost:1313/post/2021-02-10-accepted-for-ipmi-2021-virtual/</link>
      <pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2021-02-10-accepted-for-ipmi-2021-virtual/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Received Alzheimer&#39;s Disease Research Center Developmental Project grant by Pitt ADRC for 2021-2022</title>
      <link>http://localhost:1313/post/2020-12-16-received-alzheimers-disease-research-center-developmental-project-grant-by-pitt-adrc-for-2021-2022/</link>
      <pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2020-12-16-received-alzheimers-disease-research-center-developmental-project-grant-by-pitt-adrc-for-2021-2022/</guid>
      <description>&lt;p&gt;A bias-resilient deep learning algorithm for robust white matter hyperintensity segmentation on Alzheimer’s disease data with confounding factors&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Bye</title>
      <link>http://localhost:1313/post/20-12-02-icml-best-paper/</link>
      <pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/20-12-02-icml-best-paper/</guid>
      <description>&lt;p&gt;Congratulations to Jian Yang and Monica Hall for winning the Best Paper Award at the 2020 Conference on Wowchemy for their paper “Learning Wowchemy”.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hello</title>
      <link>http://localhost:1313/post/20-12-01-wowchemy-prize/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/20-12-01-wowchemy-prize/</guid>
      <description>&lt;p&gt;Congratulations to Richard Hendricks for winning first place in the Wowchemy Prize.&lt;/p&gt;
&lt;p&gt;Explain&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accepted for The Anatomical Record</title>
      <link>http://localhost:1313/post/2020-10-23-accepted-for-the-anatomical-record/</link>
      <pubDate>Fri, 23 Oct 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2020-10-23-accepted-for-the-anatomical-record/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accepted for an Oral Presentation at The Workshop on BioImage Computing @ ECCV 2020</title>
      <link>http://localhost:1313/post/2020-07-02-accepted-for-an-oral-presentation-at-the-workshop-on-bioimage-computing-@-eccv-2020/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2020-07-02-accepted-for-an-oral-presentation-at-the-workshop-on-bioimage-computing-@-eccv-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Xingchen Zhao received the SCI Undergraduate Research Scholars award for Summer 2020</title>
      <link>http://localhost:1313/post/2020-04-22-xingchen-zhao-received-the-sci-undergraduate-research-scholars-award-for-summer-2020/</link>
      <pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2020-04-22-xingchen-zhao-received-the-sci-undergraduate-research-scholars-award-for-summer-2020/</guid>
      <description>&lt;p&gt;Congratulations!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accepted for ISBI 2020, Iowa City, USA</title>
      <link>http://localhost:1313/post/2020-01-03-accepted-for-isbi-2020-iowa-city-usa/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2020-01-03-accepted-for-isbi-2020-iowa-city-usa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Joining as an Assistant Professor in the Department of Computer Science at the University of Pittsburgh</title>
      <link>http://localhost:1313/post/2019-10-19-joining-as-an-assistant-professor-in-the-department-of-computer-science-at-the-university-of-pittsburgh/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2019-10-19-joining-as-an-assistant-professor-in-the-department-of-computer-science-at-the-university-of-pittsburgh/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>http://localhost:1313/publication-dummy/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication-dummy/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Add the publication&amp;rsquo;s &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; here. You can use rich formatting such as including &lt;a href=&#34;https://docs.hugoblox.com/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>http://localhost:1313/publication-dummy/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication-dummy/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Add the publication&amp;rsquo;s &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; here. You can use rich formatting such as including &lt;a href=&#34;https://docs.hugoblox.com/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>http://localhost:1313/publication-dummy/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication-dummy/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Add the publication&amp;rsquo;s &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; here. You can use rich formatting such as including &lt;a href=&#34;https://docs.hugoblox.com/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/cobra2024/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/cobra2024/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;meta charset=&#34;utf-8&#34;&gt;
  &lt;meta name=&#34;description&#34;
        content=&#34;Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.&#34;&gt;
  &lt;meta name=&#34;keywords&#34; content=&#34;Nerfies, D-NeRF, NeRF&#34;&gt;
  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34;&gt;
  &lt;title&gt;CoBra&lt;/title&gt;

  &lt;!-- Global site tag (gtag.js) - Google Analytics --&gt;
  &lt;script async src=&#34;https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL&#34;&gt;&lt;/script&gt;
  &lt;script&gt;
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag(&#39;js&#39;, new Date());

    gtag(&#39;config&#39;, &#39;G-PYVRSFMDRL&#39;);
  &lt;/script&gt;
 &lt;script type=&#34;text/x-mathjax-config&#34;&gt;
        MathJax.Hub.Config({            
            tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}            
        });
    &lt;/script&gt;
  &lt;script src=&#39;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML&#39; async&gt;&lt;/script&gt;
  &lt;link href=&#34;https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro&#34;
        rel=&#34;stylesheet&#34;&gt;

  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/bulma.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/bulma-carousel.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/bulma-slider.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/fontawesome.all.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34;
        href=&#34;https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/index.css&#34;&gt;
  &lt;link rel=&#34;icon&#34; href=&#34;./static/images/favicon.svg&#34;&gt;

  &lt;script src=&#34;https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js&#34;&gt;&lt;/script&gt;
  &lt;script defer src=&#34;./static/js/fontawesome.all.min.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;./static/js/bulma-carousel.min.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;./static/js/bulma-slider.min.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;./static/js/index.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;&gt;&lt;/script&gt;
  &lt;script id=&#34;MathJax-script&#34; async src=&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#34;&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;nav class=&#34;navbar&#34; role=&#34;navigation&#34; aria-label=&#34;main navigation&#34;&gt;
  &lt;div class=&#34;navbar-brand&#34;&gt;
    &lt;a role=&#34;button&#34; class=&#34;navbar-burger&#34; aria-label=&#34;menu&#34; aria-expanded=&#34;false&#34;&gt;
      &lt;span aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
      &lt;span aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
      &lt;span aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
    &lt;/a&gt;
  &lt;/div&gt;

&lt;/nav&gt;


&lt;section class=&#34;hero&#34;&gt;
  &lt;div class=&#34;hero-body&#34;&gt;
    &lt;div class=&#34;container is-max-desktop&#34;&gt;
      &lt;div class=&#34;columns is-centered&#34;&gt;
        &lt;div class=&#34;column has-text-centered&#34;&gt;
          &lt;h1 class=&#34;title is-2 publication-title&#34;&gt;CoBra: Complementary Branch Fusing Class and Semantic Knowledge for Robust Weakly Supervised Semantic Segmentation&lt;/h1&gt;
          &lt;div class=&#34;is-size-5 publication-authors&#34;&gt;

            &lt;span class=&#34;author-block&#34;&gt;
              &lt;a&gt;code&lt;/a&gt;
            &lt;/span&gt;
          &lt;/div&gt;

  

          &lt;div class=&#34;column has-text-centered&#34;&gt;
            &lt;div class=&#34;publication-links&#34;&gt;
   
              &lt;span class=&#34;link-block&#34;&gt;
                &lt;a href=&#34;https://github.com/MICV-yonsei/Cobra&#34;
                   class=&#34;external-link button is-normal is-rounded is-dark&#34;&gt;
                  &lt;span class=&#34;icon&#34;&gt;
                      &lt;i class=&#34;fab fa-github&#34;&gt;&lt;/i&gt;
                  &lt;/span&gt;
                  &lt;span&gt;Code&lt;/span&gt;
                  &lt;/a&gt;
              &lt;/span&gt;

            &lt;/div&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

&lt;section class=&#34;hero teaser&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;div class=&#34;hero-body&#34;&gt;
      &lt;img src=&#34;./static/images/web0.png&#34; alt=&#34;main&#34; height=&#34;100%&#34;&gt;
      &lt;h2 class=&#34;subtitle has-text-centered&#34;&gt;
        Overview illustration of our model, &lt;b&gt;Co&lt;/b&gt;mplementary &lt;b&gt;Bra&lt;/b&gt;nch (&lt;b&gt;CoBra&lt;/b&gt;).  &lt;br&gt;        
        The dual branch framework consists of the Class-Aware Knowledge branch with CNN and the Semantic-Aware Knowledge branch with ViT. They give complementary knowledge to each branch.
        &lt;/h2&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;!-- Motivation. --&gt;
    &lt;div class=&#34;columns is-centered has-text-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h2 class=&#34;title is-3&#34;&gt;Motivation&lt;/h2&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          &lt;img src=&#34;./static/images/motivation.png&#34; alt=&#34;main&#34; height=&#34;100%&#34;&gt;
          While Class Activation Maps (CAMs) using CNNs have steadily been contributing to the success of WSSS, the resulting activation maps often narrowly focus on class-specific parts (e.g., only face of human). On the other hand, recent works based on vision transformers (ViT) have shown promising results based on their self-attention mechanism to capture the semantic parts but fail in capturing complete class-specific details (e.g., entire body parts of human but also with a dog nearby).  &lt;br&gt;
         The figure shows the comparison of object localization maps from each CNN, ViT, and Cobra branches for various subjects (human, dog, airplane), illustrating the distinctive areas of interest each model identifies. Our model successfully utilizes &lt;b&gt;complementary characteristics to localize the exact object of the correct class and its semantic parts.&lt;/b&gt;
          
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

  
&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;!-- Key Contribution. --&gt;
    &lt;div class=&#34;columns is-centered has-text-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h2 class=&#34;title is-3&#34;&gt;Key Contribution&lt;/h2&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          &lt;li&gt;We propose a dual branch framework, namely &lt;b&gt;Co&lt;/b&gt;mplementary &lt;b&gt;Bra&lt;/b&gt;nch (&lt;b&gt;CoBra&lt;/b&gt;), which aims to fuse the complementary nature of CNN and ViT localization maps. &lt;/li&gt;
          &lt;li&gt;We capture the class and semantic knowledge as Class Aware Projection (CAP) and Semantic-Aware Projection (SAP) respectively for effective complementary guidance to the CNN and ViT branches in CoBra, employing contrastive learning for enhanced guidance. &lt;/li&gt;
          &lt;li&gt;Extensive experiments qualitatively and quantitatively investigate how CNN and ViT complement each other on the PASCAL VOC 2012 dataset and MS COCO 2014 dataset, showing a state-of-the-art WSSS result. &lt;/li&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

  &lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;

    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Main model&lt;/h3&gt;
          &lt;div class=&#34;content has-text-centered&#34;&gt;
            &lt;img src=&#34;./static/images/main.png&#34; alt=&#34;Main figure&#34;&gt;
          &lt;/div&gt;
          &lt;div class=&#34;content has-text-justified&#34;&gt;
            &lt;p&gt;
              Overview illustration of our model. &lt;br&gt;
      &lt;b&gt;(I) Class Aware Knoweldge(CAK)&lt;/b&gt;: The CNN outputs a feature map which generates (1) CNN CAMs via $f_{CAM}$, (2) Pseudo-Labels from CNN CAMs via $argmax$, and (3) Class-Aware Projection (CAP) via $f_{proj}$. &lt;br&gt;
              
      &lt;b&gt;(II) Semantic Aware Knowledge(SAK)&lt;/b&gt;: The ViT outputs $N^2$ Patch Embeddings which generate (1) ViT CAMs via $f_{CAM}$ and (2) Semantic-Aware Projection (SAP) via $f_{proj}$. We also use the Attention Maps of all $L$-layers to generate (3) Patch Affinity of size $N^2 \times N^2$.
            
            &lt;/p&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    
  &lt;/div&gt;
&lt;/section&gt;
  
  
&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;

    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Method&lt;/h3&gt;
          &lt;div class=&#34;content has-text-centered&#34;&gt;
            &lt;img src=&#34;./static/images/main2.png&#34; alt=&#34;Main figure&#34;&gt;
          &lt;/div&gt;
          &lt;div class=&#34;content has-text-justified&#34;&gt;
            &lt;p&gt;
              Illustration of refining CAP and SAP from SAK and CAK branch respectively. &lt;br&gt;
      &lt;b&gt;(I) Class Aware Knoweldge(CAK)&lt;/b&gt;: The CAP values are embedded in the Class Feature Space. (1) The patch affinity from SAK branch assigns the positive (green), negative (red), and neutral (teal) patches based on the target (white) patch. (2) The CNN CAM shows that the false negative patches have been weakly localized as horse. (3) The CAP loss pull those weakly localized patches (i.e., false class negatives) since they are assigned as semantically positive patches based on SAK branch. (3) The CAP is refined to improve the CNN CAM showing fewer false class negatives. &lt;br&gt;
      &lt;b&gt;(II) Semantic Aware Knowledge(SAK)&lt;/b&gt;: The SAP values are embedded in the Semantic Feature Space. (1) The CNN CAM from CAK branch assigns the positive (green), negative (red), and neutral (teal) patches based on the target (white) patch. (2) The ViT CAM shows that the negative patches have been incorrectly localized as horse. The SAP loss pushes away those incorrectly localized patches (i.e., false class positives) since they are assigned as negative patches based on CAK branch. (3) The SAP is refined to improve the ViT CAM showing fewer false class positives.
            &lt;/p&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    
  &lt;/div&gt;
&lt;/section&gt;


&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;

    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Quantitative Experiments&lt;/h3&gt;
        
        &lt;h4 class=&#34;title is-3 has-text-centered&#34;&gt;Pascal VOC 2012 seed &amp; mask results&lt;/h3&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;img src=&#34;./static/images/table1.png&#34; alt=&#34;Main figure&#34;&gt;
        &lt;/div&gt;
        &lt;p&gt;
       Evaluation of initial seed and corresponding pseudo segmentation mask on PASCAL VOC 2012 training set in mIoU (%).
        &lt;/p&gt;
        &lt;br&gt;
        &lt;br&gt;
        &lt;h4 class=&#34;title is-3 has-text-centered&#34;&gt;Pascal VOC 2012 segmentation results&lt;/h3&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;img src=&#34;./static/images/table2.png&#34; alt=&#34;Main figure&#34;&gt;
        &lt;/div&gt;
        &lt;p&gt;
        Semantic segmentation results on the validation (Val) and Test set of PASCAL VOC 2012 dataset. Sup. (Supervision) : Image (I) and Saliency Map (S).
        &lt;/p&gt;
        &lt;br&gt;
        &lt;br&gt;
        &lt;h4 class=&#34;title is-3 has-text-centered&#34;&gt;MS-COCO 2014 segmentation results&lt;/h3&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;img src=&#34;./static/images/Table3.png&#34; alt=&#34;Main figure&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
        &lt;p&gt;
        Semgentation mIoU results(%) on MS-COCO 2014 val dataset
          &lt;br&gt;
          &lt;br&gt;
        &lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    
  &lt;/div&gt;
&lt;/section&gt;
  
&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;

    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Qualitative Experiments&lt;/h3&gt;
        
        &lt;h4 class=&#34;title is-3 has-text-centered&#34;&gt;Seed Results&lt;/h4&gt;
        
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;img src=&#34;./static/images/main3.png&#34; alt=&#34;Main figure&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          &lt;p&gt;
            Qualitative results. From left: (1) Input image, (2) Our result, (3) CNN CAM of our model, (4) Ours without SAP Loss, (5) ViT CAM of our model, (6) Ours without CAP Loss, (7) Our Pseudo mask for segmentation and (8) ground-truth segmentation label. We see that our results are able to differentiate between classes while finding their accurate object boundaries.
          &lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    
  &lt;/div&gt;
&lt;/section&gt;
  
&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;

    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Pascal VOC Segmentation Results&lt;/h3&gt;
        
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;img src=&#34;./static/images/pascal.png&#34; alt=&#34;Main figure&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          &lt;p&gt;
            Qualitative seg results on the PASCAL VOC val set.
          &lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    
  &lt;/div&gt;
&lt;/section&gt;


&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;

    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;MS COCO Segmentation Results&lt;/h3&gt;
        
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;img src=&#34;./static/images/coco.png&#34; alt=&#34;Main figure&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          &lt;p&gt;
            Qualitative seg results on the MS COCO val set.
          &lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    
  &lt;/div&gt;
&lt;/section&gt;
  

&lt;/section&gt;


&lt;footer class=&#34;footer&#34;&gt;
  &lt;div class=&#34;container&#34;&gt;

    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-8&#34;&gt;
        &lt;div class=&#34;content&#34;&gt;
          &lt;p&gt;
            This website is licensed under a &lt;a rel=&#34;license&#34;
                                                href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative
            Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.
          &lt;/p&gt;
          &lt;p&gt;
            This means you are free to borrow the &lt;a
              href=&#34;https://github.com/nerfies/nerfies.github.io&#34;&gt;source code&lt;/a&gt; of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          &lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/footer&gt;

&lt;/body&gt;
&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/cxrl2024/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/cxrl2024/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;meta charset=&#34;utf-8&#34;&gt;
  &lt;meta name=&#34;description&#34;
        content=&#34;Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.&#34;&gt;
  &lt;meta name=&#34;keywords&#34; content=&#34;Nerfies, D-NeRF, NeRF&#34;&gt;
  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34;&gt;
  &lt;title&gt;CXRL&lt;/title&gt;

  &lt;!-- Global site tag (gtag.js) - Google Analytics --&gt;
  &lt;script async src=&#34;https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL&#34;&gt;&lt;/script&gt;
  &lt;script type=&#34;text/javascript&#34; async
    src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
  &lt;/script&gt;

  &lt;script&gt;
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag(&#39;js&#39;, new Date());

    gtag(&#39;config&#39;, &#39;G-PYVRSFMDRL&#39;);
  &lt;/script&gt;

  &lt;link href=&#34;https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro&#34;
        rel=&#34;stylesheet&#34;&gt;

  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/bulma.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/bulma-carousel.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/bulma-slider.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/fontawesome.all.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34;
        href=&#34;https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/index.css&#34;&gt;
  &lt;link rel=&#34;icon&#34; href=&#34;./static/images/favicon.svg&#34;&gt;

  &lt;script src=&#34;https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js&#34;&gt;&lt;/script&gt;
  &lt;script defer src=&#34;./static/js/fontawesome.all.min.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;./static/js/bulma-carousel.min.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;./static/js/bulma-slider.min.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;./static/js/index.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;&gt;&lt;/script&gt;
  &lt;script id=&#34;MathJax-script&#34; async src=&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#34;&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;nav class=&#34;navbar&#34; role=&#34;navigation&#34; aria-label=&#34;main navigation&#34;&gt;
  &lt;div class=&#34;navbar-brand&#34;&gt;
    &lt;a role=&#34;button&#34; class=&#34;navbar-burger&#34; aria-label=&#34;menu&#34; aria-expanded=&#34;false&#34;&gt;
      &lt;span aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
      &lt;span aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
      &lt;span aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
    &lt;/a&gt;
  &lt;/div&gt;

&lt;/nav&gt;


&lt;section class=&#34;hero&#34;&gt;
  &lt;div class=&#34;hero-body&#34;&gt;
    &lt;div class=&#34;container is-max-desktop&#34;&gt;
      &lt;div class=&#34;columns is-centered&#34;&gt;
        &lt;div class=&#34;column has-text-centered&#34;&gt;
          &lt;h1 class=&#34;title is-2 publication-title&#34;&gt;Advancing Text-Driven Chest X-Ray Generation with Policy-Based Reinforcement Learning&lt;/h1&gt;
            

            &lt;div class=&#34;is-size-4 publication-authors&#34;&gt;
              &lt;strong&gt;&lt;span class=&#34;author-block&#34;&gt;Early Accept @ MICCAI 2024&lt;/span&gt;&lt;/strong&gt;
            &lt;/div&gt;
            &lt;div class=&#34;is-size-4 publication-authors&#34;&gt;
              &lt;span class=&#34;author-block&#34;&gt;
                Woojung Han&lt;sup&gt;*&lt;/sup&gt;,&lt;/span&gt;
              &lt;span class=&#34;author-block&#34;&gt;
                Chanyoung Kim&lt;sup&gt;*&lt;/sup&gt;,&lt;/span&gt;
              &lt;span class=&#34;author-block&#34;&gt;
                Dayun Ju&lt;/a&gt;,
              &lt;/span&gt;
              &lt;span class=&#34;author-block&#34;&gt;
                Yumin Shim&lt;/a&gt;,
              &lt;/span&gt;
              &lt;span class=&#34;author-block&#34;&gt;
                Seong Jae Hwang
              &lt;/span&gt;
            &lt;/div&gt;

            &lt;div class=&#34;is-size-4 publication-authors&#34;&gt;
              &lt;span class=&#34;author-block&#34;&gt;Yonsei University&lt;/span&gt;
            &lt;/div&gt;

        &lt;/div&gt;

        
      &lt;/div&gt;

      &lt;div class=&#34;column has-text-centered&#34;&gt;
        &lt;div class=&#34;publication-links&#34;&gt;
          &lt;!-- PDF Link. --&gt;
          &lt;span class=&#34;link-block&#34;&gt;
            &lt;a href=&#34;https://arxiv.org/abs/2403.06516&#34;
               class=&#34;external-link button is-normal is-rounded is-dark&#34;&gt;
              &lt;span class=&#34;icon&#34;&gt;
                  &lt;i class=&#34;fas fa-file-pdf&#34;&gt;&lt;/i&gt;
              &lt;/span&gt;
              &lt;span&gt;Paper&lt;/span&gt;
            &lt;/a&gt;
          &lt;/span&gt;
          &lt;span class=&#34;link-block&#34;&gt;
            &lt;a href=&#34;https://arxiv.org/abs/2403.06516&#34;
               class=&#34;external-link button is-normal is-rounded is-dark&#34;&gt;
              &lt;span class=&#34;icon&#34;&gt;
                  &lt;i class=&#34;ai ai-arxiv&#34;&gt;&lt;/i&gt;
              &lt;/span&gt;
              &lt;span&gt;arXiv&lt;/span&gt;
            &lt;/a&gt;
          &lt;/span&gt;
          &lt;!-- Code Link. --&gt;
          &lt;span class=&#34;link-block&#34;&gt;
            &lt;a href=&#34;https://github.com/MICV-yonsei/CXRL&#34;
               class=&#34;external-link button is-normal is-rounded is-dark&#34;&gt;
              &lt;span class=&#34;icon&#34;&gt;
                  &lt;i class=&#34;fab fa-github&#34;&gt;&lt;/i&gt;
              &lt;/span&gt;
              &lt;span&gt;Code&lt;/span&gt;
              &lt;/a&gt;
          &lt;/span&gt;
          
        &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

&lt;section class=&#34;hero teaser&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;div class=&#34;hero-body&#34;&gt;
      &lt;div class=&#34;image-at-center&#34;&gt;
        &lt;img src=&#34;./static/images/overview.png&#34; alt=&#34;Overview of CXRL&#34; width=&#34;80%&#34;&gt;
      &lt;/div&gt;
      &lt;br&gt;
      &lt;h2 class=&#34;subtitle has-text-centered&#34;&gt;
        We introduce &lt;b&gt;&lt;i&gt;CXRL&lt;/i&gt;&lt;/b&gt;,
        &lt;br&gt;Advancing Text-Driven Chest X-Ray Generation with Policy-Based Reinforcement Learning.

      &lt;/h2&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;!-- Abstract. --&gt;
    &lt;div class=&#34;columns is-centered has-text-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h2 class=&#34;title is-3&#34;&gt;Abstract&lt;/h2&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          Recent advances in text-conditioned image generation diffusion models have begun paving the way for new opportunities in modern medical domain, in particular, generating Chest X-rays (CXRs) from diagnostic reports. 
          Nonetheless, to further drive the diffusion models to generate CXRs that faithfully reflect the complexity and diversity of real data, it has become evident that a nontrivial learning approach is needed. 
          In light of this, we propose CXRL, a framework motivated by the potential of reinforcement learning (RL). 
          Specifically, we integrate a policy gradient RL approach with well-designed multiple distinctive CXR-domain specific reward models. 
          This approach guides the diffusion denoising trajectory, achieving precise CXR posture and pathological details. 
          Here, considering the complex medical image environment, we present “RL with Comparative Feedback” (RLCF) for the reward mechanism, a human-like comparative evaluation that is known to be more effective and reliable in complex scenarios compared to direct evaluation. 
          Our CXRL framework includes jointly optimizing learnable adaptive condition embeddings (ACE) and the image generator, enabling the model to produce more accurate and higher perceptual CXR quality. Our extensive evaluation of the MIMIC-CXR-JPG dataset demonstrates the effectiveness of our RL-based tuning approach. Consequently, our CXRL generates pathologically realistic CXRs, establishing a new standard for generating CXRs with high fidelity to real-world clinical scenarios.
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;

    &lt;br&gt;&lt;br&gt;&lt;br&gt;

    &lt;div class=&#34;columns is-centered has-text-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h2 class=&#34;title is-3&#34;&gt;Video&lt;/h2&gt;
        &lt;div class=&#34;publication-video&#34;&gt;
          &lt;!-- &lt;video src=&#34;static/images/eagle_video.mp4&#34; controls&gt;&lt;/video&gt; --&gt;
          &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/4MWTUwFJm8I?si=5bRP1Pvfn0JpuvTz&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;

  &lt;/div&gt;
&lt;/section&gt;

&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Method&lt;/h3&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Pipeline&lt;/h4&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          &lt;img src=&#34;./static/images/overview.png&#34; alt=&#34;Main figure&#34;&gt;
          &lt;br&gt;
          &lt;p&gt;
            The pipeline of &lt;i&gt;CXRL&lt;/i&gt;. 
            Our model employs policy gradient optimization utilizing multi-reward feedback, fine-tuning image generator, and ACE to produce realistic and accurate CXR that corresponds closely to the input report.
            &lt;br&gt;
            &lt;b&gt;Contribution&lt;/b&gt;
            &lt;br&gt;
            &lt;ul&gt;
              &lt;li&gt;Our study pioneers in applying RL to text-conditioned medical image synthesis, particularly in CXRs, focusing on detail refinement and input condition control for clinical accuracy.&lt;/li&gt;
              &lt;li&gt;We advance report-to-CXR generation with an RLCF-based rewarding mechanism, emphasizing posture alignment, pathology accuracy, and consistency between input reports and generated CXRs.&lt;/li&gt;
              &lt;li&gt;We jointly optimize the image generator and ACE via reward feedback models, ensuring image-text alignment and medical accuracy across varied reports, setting a new benchmark in a report-to-CXR generation.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/p&gt;
        &lt;/div&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Reward Feedback Models&lt;/h4&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          &lt;img src=&#34;./static/images/feedback.png&#34; alt=&#34;Main figure&#34;&gt;
          &lt;p&gt;
            A detailed illustration of our reward feedback models. We incorporate three different feedbacks for report-to-CXR generation model to generate goal-oriented CXRs.
            &lt;ul&gt;
               &lt;li&gt;&lt;b&gt;Posture Alignment Feedback&lt;/b&gt;: Generated CXRs often face scaling issues, like excessive zooming or rotation, obscuring essential details. To counter these undesirable effects, we introduce a reward signal to align the CXR&#39;s posture with a canonical orientation to preserve essential parts.&lt;/li&gt;
               &lt;li&gt;&lt;b&gt;Diagnostic Condition Feedback&lt;/b&gt;: To accurately reflect generated CXRs with referenced pathologies, we classify them using a parsed report label, rewarding its accuracy.&lt;/li&gt;
               &lt;li&gt;&lt;b&gt;Multimodal Consistency Feedback&lt;/b&gt;: We enforce the generated CXRs to better match their reports. We leverage a multimodal latent representation pretrained with CXR-report pairs for semantic agreement assessment.&lt;/li&gt;
             &lt;/ul&gt;
          &lt;/p&gt;
        &lt;/div&gt;
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;

    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Qualitative Results&lt;/h3&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Comparison between previous models and ours&lt;/h4&gt;
        &lt;section class=&#34;hero is-light is-small&#34;&gt;
          &lt;div class=&#34;content has-text-centered&#34;&gt;
            &lt;img src=&#34;./static/images/result1.png&#34; alt=&#34;Qualitative ablation&#34;&gt;
          &lt;/div&gt;
        &lt;/section&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;p&gt;
             Comparison between previous state-of-the-art report-to-CXR generation models [19,3] and ours. The blue and green texts match their corresponding colored arrows.
          &lt;/p&gt;
        &lt;/div&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Additional Qualitative results&lt;/h4&gt;
        &lt;section class=&#34;hero is-light is-small&#34;&gt;
          &lt;div class=&#34;content has-text-centered&#34;&gt;
            &lt;img src=&#34;./static/images/result2.png&#34; alt=&#34;Qualitative ablation&#34;&gt;
          &lt;/div&gt;
        &lt;/section&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;p&gt;
            Additional qualitative results of our framework comparing against baselines. The colored texts match their corresponding colored arrows. Ours w/o ACE or RLCF demonstrates superior report agreement and posture alignment compared to other baselines. CXRL is observed to generate more advanced high-fidelity CXRs that highlight our methodology&#39;s effectiveness in synthesizing clinically accurate medical images.
          &lt;/p&gt;
        &lt;/div&gt;

        
        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Qualitative ablation on each reward model&lt;/h4&gt;
        &lt;section class=&#34;hero is-light is-small&#34;&gt;
          &lt;div class=&#34;content has-text-centered&#34;&gt;
            &lt;img src=&#34;./static/images/ablation.png&#34; alt=&#34;Qualitative ablation&#34;&gt;
          &lt;/div&gt;
        &lt;/section&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;p&gt;
             &lt;ul&gt;
               &lt;li&gt;(a): CXRL shows significantly better alignment of the &lt;i&gt;clavicle&lt;/i&gt; and &lt;i&gt;costophrenic angle&lt;/i&gt; compared to the anchor regarding posture alignment.&lt;/li&gt;
               &lt;li&gt;(b): CXRL demonstrates improved predictive diagnostic accuracy, closely matching the GT and enhancing clinical decision-making&lt;/li&gt;
               &lt;li&gt;(c): The multimodal consistency reward ensures that CXRs and reports correspond well, as observed by arrows and text in matching colors.&lt;/li&gt;
             &lt;/ul&gt;
          &lt;/p&gt;
        &lt;/div&gt;

      &lt;/div&gt;
    &lt;/div&gt;
    
  &lt;/div&gt;
&lt;/section&gt;


&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Evaluation of generated CXRs from multiple feedback perspectives&lt;/h3&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Evaluation Metrics&lt;/h4&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;img src=&#34;./static/images/table1.png&#34; alt=&#34;three-evaluation&#34; width=&#34;80%&#34;&gt;
          &lt;p&gt;
            The table compares the performance of various methods using three evaluation metrics.
          &lt;/p&gt;
        &lt;/div&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;CXR Quality Table&lt;/h4&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;img src=&#34;./static/images/table2.png&#34; alt=&#34;ablation&#34; width=&#34;80%&#34;&gt;
          &lt;p&gt;
            Comparative analysis of generated CXR quality: (a) quantitatively compares established models using FID and MS-SSIM metrics; (b) evaluates the impact of reward components on FID scores.
          &lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

&lt;section class=&#34;section&#34; id=&#34;BibTeX&#34;&gt;
  &lt;div class=&#34;container is-max-desktop content&#34;&gt;
    &lt;h2 class=&#34;title&#34;&gt;BibTeX&lt;/h2&gt;
    &lt;pre&gt;&lt;code&gt;@InProceedings{2024cxrl,
      author    = {Han, Woojung and Kim, Chanyoung and Ju, Dayun and Shim, Yumin and Hwang, Seong Jae},
      title     = {Advancing Text-Driven Chest X-Ray Generation with Policy-Based Reinforcement Learning},
      booktitle = {Medical Image Computing and Computer Assisted Intervention (MICCAI)},
      month     = {Oct},
      year      = {2024}
}&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;
&lt;/section&gt;


&lt;footer class=&#34;footer&#34;&gt;
  &lt;div class=&#34;container&#34;&gt;

    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-8&#34;&gt;
        &lt;div class=&#34;content&#34;&gt;
          &lt;p&gt;
            This website is licensed under a &lt;a rel=&#34;license&#34;
                                                href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative
            Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.
          &lt;/p&gt;
          &lt;p&gt;
            This means you are free to borrow the &lt;a
              href=&#34;https://github.com/nerfies/nerfies.github.io&#34;&gt;source code&lt;/a&gt; of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          &lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/footer&gt;

&lt;script src=&#34;./static/js/tag-height.js&#34;&gt;&lt;/script&gt;

&lt;/body&gt;
&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/eagle2024/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/eagle2024/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;meta charset=&#34;utf-8&#34;&gt;
  &lt;meta name=&#34;description&#34;
        content=&#34;Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.&#34;&gt;
  &lt;meta name=&#34;keywords&#34; content=&#34;Nerfies, D-NeRF, NeRF&#34;&gt;
  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34;&gt;
  &lt;title&gt;EAGLE&lt;/title&gt;

  &lt;!-- Global site tag (gtag.js) - Google Analytics --&gt;
  &lt;script async src=&#34;https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL&#34;&gt;&lt;/script&gt;
  &lt;script type=&#34;text/javascript&#34; async
    src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
  &lt;/script&gt;

  &lt;script&gt;
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag(&#39;js&#39;, new Date());

    gtag(&#39;config&#39;, &#39;G-PYVRSFMDRL&#39;);
  &lt;/script&gt;

  &lt;link href=&#34;https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro&#34;
        rel=&#34;stylesheet&#34;&gt;

  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/bulma.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/bulma-carousel.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/bulma-slider.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/fontawesome.all.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34;
        href=&#34;https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css&#34;&gt;
  &lt;link rel=&#34;stylesheet&#34; href=&#34;./static/css/index.css&#34;&gt;
  &lt;link rel=&#34;icon&#34; href=&#34;./static/images/favicon.svg&#34;&gt;

  &lt;script src=&#34;https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js&#34;&gt;&lt;/script&gt;
  &lt;script defer src=&#34;./static/js/fontawesome.all.min.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;./static/js/bulma-carousel.min.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;./static/js/bulma-slider.min.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;./static/js/index.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;&gt;&lt;/script&gt;
  &lt;script id=&#34;MathJax-script&#34; async src=&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#34;&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;nav class=&#34;navbar&#34; role=&#34;navigation&#34; aria-label=&#34;main navigation&#34;&gt;
  &lt;div class=&#34;navbar-brand&#34;&gt;
    &lt;a role=&#34;button&#34; class=&#34;navbar-burger&#34; aria-label=&#34;menu&#34; aria-expanded=&#34;false&#34;&gt;
      &lt;span aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
      &lt;span aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
      &lt;span aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
    &lt;/a&gt;
  &lt;/div&gt;

&lt;/nav&gt;


&lt;section class=&#34;hero&#34;&gt;
  &lt;div class=&#34;hero-body&#34;&gt;
    &lt;div class=&#34;container is-max-desktop&#34;&gt;
      &lt;div class=&#34;columns is-centered&#34;&gt;
        &lt;div class=&#34;column has-text-centered&#34;&gt;
          &lt;h1 class=&#34;title is-2 publication-title&#34;&gt;EAGLE🦅: Eigen Aggregation Learning for Object-Centric
            Unsupervised Semantic Segmentation&lt;/h1&gt;
            

            &lt;div class=&#34;is-size-4 publication-authors&#34;&gt;
              &lt;strong&gt;&lt;span class=&#34;author-block&#34;&gt;Highlight @ CVPR 2024&lt;/span&gt;&lt;/strong&gt;
            &lt;/div&gt;
            &lt;div class=&#34;is-size-4 publication-authors&#34;&gt;
              &lt;span class=&#34;author-block&#34;&gt;
                Chanyoung Kim&lt;sup&gt;*&lt;/sup&gt;,&lt;/span&gt;
              &lt;span class=&#34;author-block&#34;&gt;
                Woojung Han&lt;sup&gt;*&lt;/sup&gt;,&lt;/span&gt;
              &lt;span class=&#34;author-block&#34;&gt;
                Dayun Ju&lt;/a&gt;,
              &lt;/span&gt;
              &lt;span class=&#34;author-block&#34;&gt;
                Seong Jae Hwang
              &lt;/span&gt;
            &lt;/div&gt;

            &lt;div class=&#34;is-size-4 publication-authors&#34;&gt;
              &lt;span class=&#34;author-block&#34;&gt;Yonsei University&lt;/span&gt;
            &lt;/div&gt;

        &lt;/div&gt;

        
      &lt;/div&gt;

      &lt;div class=&#34;column has-text-centered&#34;&gt;
        &lt;div class=&#34;publication-links&#34;&gt;
          &lt;!-- PDF Link. --&gt;
          &lt;span class=&#34;link-block&#34;&gt;
            &lt;a href=&#34;https://arxiv.org/abs/2403.01482&#34;
               class=&#34;external-link button is-normal is-rounded is-dark&#34;&gt;
              &lt;span class=&#34;icon&#34;&gt;
                  &lt;i class=&#34;fas fa-file-pdf&#34;&gt;&lt;/i&gt;
              &lt;/span&gt;
              &lt;span&gt;Paper&lt;/span&gt;
            &lt;/a&gt;
          &lt;/span&gt;
          &lt;span class=&#34;link-block&#34;&gt;
            &lt;a href=&#34;https://arxiv.org/abs/2403.01482&#34;
               class=&#34;external-link button is-normal is-rounded is-dark&#34;&gt;
              &lt;span class=&#34;icon&#34;&gt;
                  &lt;i class=&#34;ai ai-arxiv&#34;&gt;&lt;/i&gt;
              &lt;/span&gt;
              &lt;span&gt;arXiv&lt;/span&gt;
            &lt;/a&gt;
          &lt;/span&gt;
          &lt;!-- Code Link. --&gt;
          &lt;span class=&#34;link-block&#34;&gt;
            &lt;a href=&#34;https://github.com/MICV-yonsei/EAGLE&#34;
               class=&#34;external-link button is-normal is-rounded is-dark&#34;&gt;
              &lt;span class=&#34;icon&#34;&gt;
                  &lt;i class=&#34;fab fa-github&#34;&gt;&lt;/i&gt;
              &lt;/span&gt;
              &lt;span&gt;Code&lt;/span&gt;
              &lt;/a&gt;
          &lt;/span&gt;
          
        &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

&lt;section class=&#34;hero teaser&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;div class=&#34;hero-body&#34;&gt;
      &lt;div class=&#34;image-at-center&#34;&gt;
        &lt;img src=&#34;./static/images/1st_b.png&#34; alt=&#34;Visualizing EAGLE and Baselines&#34; width=&#34;80%&#34;&gt;
      &lt;/div&gt;
      &lt;br&gt;
      &lt;h2 class=&#34;subtitle has-text-centered&#34;&gt;
        We introduce &lt;b&gt;&lt;i&gt;EAGLE&lt;/i&gt;&lt;/b&gt;,
        &lt;br&gt;&lt;b&gt;E&lt;/b&gt;igen &lt;b&gt;AG&lt;/b&gt;gregation &lt;b&gt;LE&lt;/b&gt;arning for object-centric unsupervised semantic segmentation.

      &lt;/h2&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;!-- Abstract. --&gt;
    &lt;div class=&#34;columns is-centered has-text-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h2 class=&#34;title is-3&#34;&gt;Abstract&lt;/h2&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          Semantic segmentation has innately relied on extensive pixel-level labeled annotated data, leading to the emergence of unsupervised methodologies. 
          Among them, leveraging self-supervised Vision Transformers for unsupervised semantic segmentation (USS) has been making steady progress with expressive deep features. 
          Yet, for semantically segmenting images with complex objects, a predominant challenge remains: the lack of explicit object-level semantic encoding in patch-level features. 
          This technical limitation often leads to inadequate segmentation of complex objects with diverse structures. 
          To address this gap, we present a novel approach, &lt;i&gt;EAGLE&lt;/i&gt;, which emphasizes object-centric representation learning for unsupervised semantic segmentation. 
          Specifically, we introduce EiCue, a spectral technique providing semantic and structural cues through an eigenbasis derived from the semantic similarity matrix of deep image features and color affinity from an image. 
          Further, by incorporating our object-centric contrastive loss with EiCue, we guide our model to learn object-level representations with intra- and inter-image object-feature consistency, thereby enhancing semantic accuracy. 
          Extensive experiments on COCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-art USS results of EAGLE with accurate and consistent semantic segmentation across complex scenes. 
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;

    &lt;br&gt;&lt;br&gt;&lt;br&gt;

    &lt;div class=&#34;columns is-centered has-text-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h2 class=&#34;title is-3&#34;&gt;Video&lt;/h2&gt;
        &lt;div class=&#34;publication-video&#34;&gt;
          &lt;!-- &lt;video src=&#34;static/images/eagle_video.mp4&#34; controls&gt;&lt;/video&gt; --&gt;
          &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/0a799IDW4e0?si=29YK7cFgJBulyQRp&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;

  &lt;/div&gt;
&lt;/section&gt;

&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Method&lt;/h3&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Pipeline&lt;/h4&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          &lt;img src=&#34;./static/images/mainfigure.png&#34; alt=&#34;Main figure&#34;&gt;
          &lt;br&gt;
          &lt;p&gt;
            The pipeline of &lt;i&gt;EAGLE&lt;/i&gt;. 
            Leveraging the Laplacian matrix, which integrates hierarchically projected image key features and color 
            affinity, the model exploits eigenvector clustering to capture object-level perspective cues defined 
            as \( \mathrm{\mathcal{M}}_{eicue} \) and \( \mathrm{\tilde{\mathcal{M}}_{eicue}} \). 
            Distilling knowledge from \( \mathrm{\mathcal{M}}_{eicue} \), our model further adopts an 
            object-centric contrastive loss, utilizing the projected vector \( \mathrm{Z} \) and \( \mathrm{\tilde{Z}} \). 
            The learnable prototype \( \mathrm{\Phi} \) assigned from \( \mathrm{Z} \) and \( \mathrm{\tilde{Z}} \), 
            acts as a singular anchor that contrasts positive objects and negative objects. Our object-centric 
            contrastive loss is computed in two distinct manners: intra(\( \mathrm{\mathcal{L}}_{obj} \))- and 
            inter(\( \mathrm{\mathcal{L}}_{sc} \))-image to ensure semantic consistency.
          &lt;/p&gt;
        &lt;/div&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Eigen Aggregation Module&lt;/h4&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          &lt;img src=&#34;./static/images/eigenmodule.png&#34; alt=&#34;Main figure&#34;&gt;
          &lt;p&gt;
            An illustration of the EiCue generation. From the input image, both color affinity 
            matrix \( \mathrm{A_{color}} \) and semantic similarity matrix \( \mathrm{A_{seg}} \) are derived, 
            which are combined to form the Laplacian \( \mathrm{L_{sym}} \). An eigenvector 
            subset \( \mathrm{\hat{V}} \) of \( \mathrm{L_{sym}} \) are clustered to produce EiCue.
          &lt;/p&gt;
        &lt;/div&gt;
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;




&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Visualization of Primary Elements&lt;/h3&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Eigenvectors&lt;/h4&gt;
        &lt;section class=&#34;hero is-light is-small&#34;&gt;
          &lt;div class=&#34;slideContent&#34;&gt;
            &lt;div style=&#34;width: 100%;&#34;&gt;
              &lt;div id=&#34;results-carousel&#34; class=&#34;carousel results-carousel&#34;&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/vector1.png&#34; alt=&#34;Eigenvectors figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/vector2.png&#34; alt=&#34;Eigenvectors figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/vector3.png&#34; alt=&#34;Eigenvectors figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/vector4.png&#34; alt=&#34;Eigenvectors figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/vector5.png&#34; alt=&#34;Eigenvectors figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/vector6.png&#34; alt=&#34;Eigenvectors figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/vector8.png&#34; alt=&#34;Eigenvectors figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/vector9.png&#34; alt=&#34;Eigenvectors figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;

              &lt;/div&gt;
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/section&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          &lt;p&gt;
            Visualizing eigenvectors derived from \( \mathrm{S} \) in the Eigen Aggregation Module. 
            These eigenvectors not only distinguish different objects but also identify semantically related areas, 
            highlighting how EiCue captures object semantics and boundaries effectively.
          &lt;/p&gt;
        &lt;/div&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;EiCue&lt;/h4&gt;
        &lt;section class=&#34;hero is-light is-small&#34;&gt;
          &lt;div class=&#34;slideContent&#34;&gt;
            &lt;div style=&#34;width: 100%;&#34;&gt;
              &lt;div id=&#34;results-carousel&#34; class=&#34;carousel results-carousel&#34;&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/eicue1.png&#34; alt=&#34;EiCue figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/eicue2.png&#34; alt=&#34;EiCue figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/eicue3.png&#34; alt=&#34;EiCue figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/eicue4.png&#34; alt=&#34;EiCue figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/eicue5.png&#34; alt=&#34;EiCue figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/eicue6.png&#34; alt=&#34;EiCue figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/eicue7.png&#34; alt=&#34;EiCue figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/eicue8.png&#34; alt=&#34;EiCue figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/eicue9.png&#34; alt=&#34;EiCue figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;

              &lt;/div&gt;
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/section&gt;
        &lt;div class=&#34;content has-text-justified&#34;&gt;
          &lt;p&gt;
            Comparison between K-means and EiCue. 
            The bottom row presents EiCue, highlighting its superior ability to capture subtle structural 
            intricacies and understand deeper semantic relationships, which is not as effectively achieved 
            by K-means.
          &lt;/p&gt;
        &lt;/div&gt;
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;

    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Qualitative Results&lt;/h3&gt;
        
        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;COCO-Stuff&lt;/h4&gt;
        &lt;section class=&#34;hero is-light is-small&#34;&gt;
          &lt;div class=&#34;slideContent&#34;&gt;
            &lt;div style=&#34;width: 100%;&#34;&gt;
              &lt;div id=&#34;results-carousel&#34; class=&#34;carousel results-carousel&#34;&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/coco1.png&#34; alt=&#34;Coco figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/coco2.png&#34; alt=&#34;Coco figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/coco3.png&#34; alt=&#34;Coco figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/coco4.png&#34; alt=&#34;Coco figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/coco5.png&#34; alt=&#34;Coco figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
              &lt;/div&gt;
            &lt;/div&gt;
          &lt;/div&gt;
          &lt;div class=&#34;content has-text-centered&#34;&gt;
            &lt;img src=&#34;./static/images/labels_coco.png&#34; alt=&#34;Coco label&#34;&gt;
          &lt;/div&gt;
        &lt;/section&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;p&gt;
            Qualitative results of COCO-Stuff dataset trained with ViT-S/8 backbone.
          &lt;/p&gt;
        &lt;/div&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Cityscapes&lt;/h4&gt;
        &lt;section class=&#34;hero is-light is-small&#34;&gt;
          &lt;div class=&#34;slideContent&#34;&gt;
            &lt;div style=&#34;width: 100%;&#34;&gt;
              &lt;div id=&#34;results-carousel&#34; class=&#34;carousel results-carousel&#34;&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/city1.png&#34; alt=&#34;City figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/city2.png&#34; alt=&#34;City figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/city3.png&#34; alt=&#34;City figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
                &lt;div class=&#34;&#34;&gt;
                  &lt;div class=&#34;content has-text-centered&#34;&gt;
                    &lt;img src=&#34;./static/images/city4.png&#34; alt=&#34;City figure&#34;&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
              &lt;/div&gt;
            &lt;/div&gt;
          &lt;/div&gt;
          &lt;div class=&#34;content has-text-centered&#34;&gt;
            &lt;img src=&#34;./static/images/labels_city.png&#34; alt=&#34;Coco label&#34;&gt;
          &lt;/div&gt;
        &lt;/section&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;p&gt;
            Qualitative results of Cityscapes dataset trained with ViT-B/8 backbone.
          &lt;/p&gt;
        &lt;/div&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Potsdam-3&lt;/h4&gt;
        &lt;section class=&#34;hero is-light is-small&#34;&gt;
          &lt;div class=&#34;content has-text-centered&#34;&gt;
            &lt;img src=&#34;./static/images/potsdam.png&#34; alt=&#34;Potsdam supplymentary visualization&#34;&gt;
          &lt;/div&gt;
        &lt;/section&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;p&gt;
            Qualitative results of Potsdam-3 dataset trained with ViT-B/8 backbone.
          &lt;/p&gt;
        &lt;/div&gt;

      &lt;/div&gt;
    &lt;/div&gt;
    
  &lt;/div&gt;
&lt;/section&gt;


&lt;section class=&#34;section&#34;&gt;
  &lt;div class=&#34;container is-max-desktop&#34;&gt;
    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-four-fifths&#34;&gt;
        &lt;h3 class=&#34;title is-3 has-text-centered&#34;&gt;Quantitative Results&lt;/h3&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;COCO-Stuff&lt;/h4&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;img src=&#34;./static/images/table_coco.png&#34; alt=&#34;cocostuff table&#34; width=&#34;70%&#34;&gt;
          &lt;p&gt;
            Quantitative results on the COCO-Stuff dataset.
          &lt;/p&gt;
        &lt;/div&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Cityscapes&lt;/h4&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;img src=&#34;./static/images/table_city.png&#34; alt=&#34;cityscapes table&#34; width=&#34;70%&#34;&gt;
          &lt;p&gt;
            Quantitative results on the Cityscapes dataset.
          &lt;/p&gt;
        &lt;/div&gt;

        &lt;br&gt;
        &lt;h4 class=&#34;title is-4 has-text-centered&#34;&gt;Potsdam-3&lt;/h4&gt;
        &lt;div class=&#34;content has-text-centered&#34;&gt;
          &lt;img src=&#34;./static/images/table_potsdam.png&#34; alt=&#34;potsdam-3 table&#34; width=&#34;70%&#34;&gt;
          &lt;p&gt;
            Quantitative results on the Potsdam-3 dataset.
          &lt;/p&gt;
        &lt;/div&gt;
        
      &lt;/div&gt;

    &lt;/div&gt;
    
  &lt;/div&gt;

&lt;/section&gt;

&lt;section class=&#34;section&#34; id=&#34;BibTeX&#34;&gt;
  &lt;div class=&#34;container is-max-desktop content&#34;&gt;
    &lt;h2 class=&#34;title&#34;&gt;BibTeX&lt;/h2&gt;
    &lt;pre&gt;&lt;code&gt;@InProceedings{2024eagle,
      author    = {Kim, Chanyoung and Han, Woojung and Ju, Dayun and Hwang, Seong Jae},
      title     = {EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation},
      booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
      month     = {June},
      year      = {2024}
}&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;
&lt;/section&gt;


&lt;footer class=&#34;footer&#34;&gt;
  &lt;div class=&#34;container&#34;&gt;

    &lt;div class=&#34;columns is-centered&#34;&gt;
      &lt;div class=&#34;column is-8&#34;&gt;
        &lt;div class=&#34;content&#34;&gt;
          &lt;p&gt;
            This website is licensed under a &lt;a rel=&#34;license&#34;
                                                href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative
            Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.
          &lt;/p&gt;
          &lt;p&gt;
            This means you are free to borrow the &lt;a
              href=&#34;https://github.com/nerfies/nerfies.github.io&#34;&gt;source code&lt;/a&gt; of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          &lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/footer&gt;

&lt;script src=&#34;./static/js/tag-height.js&#34;&gt;&lt;/script&gt;

&lt;/body&gt;
&lt;/html&gt;
</description>
    </item>
    
  </channel>
</rss>
